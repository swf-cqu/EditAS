# encoding=utf-8

import re
from typing import List, Union

import nltk
from antlr4 import *
# use JavaLexer generated by Antlr4
# from JavaLexer import *
from utils.javatokenizer.JavaLexer import JavaLexer

"""
标识符的解析    Test的解析       Code 解析
"""

def camel_case_split(identifier):
    """
    1）先在连续的大写字母前添加空格； 2）对于含有Ab形式的添加空格
    """
    return re.sub(r'([A-Z][a-z])', r' \1', re.sub(r'([A-Z]+)', r' \1', identifier)).strip().split()


def tokenize_identifier_raw(token, keep_underscore=True):
    """
    是否根据下划线进行划分
    """
    regex = r'(_+)' if keep_underscore else r'_+'
    id_tokens = []
    for t in re.split(regex, token):
        if t:
            id_tokens += camel_case_split(t)
    # note: do not use lowercase!
    return list(filter(lambda x: len(x) > 0, id_tokens))


def tokenize_identifier(token, with_con=False):
    """
    Token 标识符
    标识符为一个整体，需要根据 1)大小写以及 2) 下划线划分 之后添加<con>连接符
    """
    if with_con:
        id_tokens = " <con> ".join(tokenize_identifier_raw(token, keep_underscore=True)).split()
    else:
        id_tokens = [t.lower() for t in tokenize_identifier_raw(token, keep_underscore=False)]
    return id_tokens


def tokenize_text(text):
    """
    对字符串进行Token化：1) nltk.word_tokenize 2) 特殊字符分割； 3) Tokenize 标识符
    """
    str_tokens = []
    nltk_tokenized = " ".join(nltk.word_tokenize(text))
    # split according to punctuations
    # string.punctuation
    # NOTE: do not care _, which will be taken care of by tokenized_identifier
    content_tokens = re.sub(r'([-!"#$%&\'()*+,./:;<=>?@\[\\\]^`{|}~])', r' \1 ', nltk_tokenized).split()
    for t in content_tokens:
        str_tokens += tokenize_identifier(t)
    return str_tokens


def tokenize_text_with_con(text):
    def _tokenize_word(word):
        new_word = re.sub(r'([-!"#$%&\'()*+,./:;<=>?@\[\\\]^`{|}~])', r' \1 ', word)  # 加了个空格
        subwords = nltk.word_tokenize(new_word)  # word tokenize
        new_subwords = []
        for w in subwords:
            new_subwords += tokenize_identifier_raw(w, keep_underscore=True)  # Tokenize identification
        return new_subwords

    tokens = []
    for word in text.split():
        if not word:
            continue
        tokens += " <con> ".join(_tokenize_word(word)).split()
    return tokens


def tokenize_string_literal(str_literal, with_con=False):
    """
    str_literal: str, STRING_LITERAL.text   #注意两个引号
    return: list of tokens

    划分是需要 特殊符号  + word token + 标识符划分  不涉及空格符号
    """
    if with_con:
        str_tokens = tokenize_text_with_con(str_literal[1:-1])
    else:
        str_tokens = tokenize_text(str_literal[1:-1])
    return ['"'] + str_tokens + ['"']


def tokenize_java_code_origin(code_str) -> List[Token]:
    """
    get java tokens and return the original CommonTokens   返回的是原始的 Java Code Token
    """
    input_stream = InputStream(code_str)
    lexer = JavaLexer(input_stream)
    tokens = []
    while True:
        t = lexer.nextToken()
        if t.text == '<EOF>':
            break
        tokens.append(t)
    return tokens


def tokenize_java_code_raw(code_str: Union[str, List[Token]]) -> List[Token]:  # code_str 可以是str 或者 List<Token> 类型 raw 原生的意思
    """
    减少一些复合词的使用
    get java tokens without splitting compound words #不分离复合词
    """
    literal_mapping = {
        JavaLexer.DECIMAL_LITERAL: "DECIMAL_LITERAL",
        JavaLexer.HEX_LITERAL: "HEX_LITERAL",
        JavaLexer.OCT_LITERAL: "OCT_LITERAL",
        JavaLexer.BINARY_LITERAL: "BINARY_LITERAL",
        JavaLexer.FLOAT_LITERAL: "FLOAT_LITERAL",
        JavaLexer.HEX_FLOAT_LITERAL: "HEX_FLOAT_LITERAL",
        JavaLexer.BOOL_LITERAL: "BOOL_LITERAL",
        JavaLexer.CHAR_LITERAL: "CHAR_LITERAL",
        JavaLexer.NULL_LITERAL: "NULL_LITERAL"
    }
    if isinstance(code_str, str):
        origin_tokens = tokenize_java_code_origin(code_str)
    else:
        origin_tokens = code_str
    new_tokens = []
    for t in origin_tokens:
        if t.type in [JavaLexer.COMMENT, JavaLexer.LINE_COMMENT, JavaLexer.WS]:
            continue
        elif t.type in list(literal_mapping.keys()):
            t.text = literal_mapping[t.type]
            new_tokens.append(t)
        else:
            # for keywords and identifiers  保留了关键字和标识符的初始版本
            new_tokens.append(t)
    return new_tokens


def tokenize_java_code(code_str, with_con=False):
    literal_mapping = {
        JavaLexer.DECIMAL_LITERAL: "DECIMAL_LITERAL",
        JavaLexer.HEX_LITERAL: "HEX_LITERAL",
        JavaLexer.OCT_LITERAL: "OCT_LITERAL",
        JavaLexer.BINARY_LITERAL: "BINARY_LITERAL",
        JavaLexer.FLOAT_LITERAL: "FLOAT_LITERAL",
        JavaLexer.HEX_FLOAT_LITERAL: "HEX_FLOAT_LITERAL",
        JavaLexer.BOOL_LITERAL: "BOOL_LITERAL",
        JavaLexer.CHAR_LITERAL: "CHAR_LITERAL",
        JavaLexer.NULL_LITERAL: "NULL_LITERAL"
    }
    origin_tokens = tokenize_java_code_origin(code_str)
    tokens = []
    for t in origin_tokens:
        if t.type in [JavaLexer.COMMENT, JavaLexer.LINE_COMMENT, JavaLexer.WS]:
            continue
        if t.type == JavaLexer.STRING_LITERAL:
            tokens += tokenize_string_literal(t.text, with_con=with_con)
        elif t.type in list(literal_mapping.keys()):
            tokens.append(literal_mapping[t.type])
        elif t.type == JavaLexer.IDENTIFIER:
            tokens += tokenize_identifier(t.text, with_con=with_con)
        else:
            # for keywords
            tokens.append(t.text)   # 关键字也被 分割了
    return tokens


